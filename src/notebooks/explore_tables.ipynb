{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a868f0",
   "metadata": {},
   "source": [
    "# Explorando as Tabelas Delta\n",
    "\n",
    "Este notebook permite explorar e manipular as tabelas Delta do projeto de diário de bordo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1252db",
   "metadata": {},
   "source": [
    "## 1. Configuração do SparkSession\n",
    "\n",
    "Configurando a sessão Spark com suporte ao Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3539879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionURL\n",
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionPassword\n",
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionDriverName\n",
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionUserName\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fa6a474d-3966-4440-a3ef-df689b3e062f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.3.0/delta-core_2.12-2.3.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;2.3.0!delta-core_2.12.jar (1124ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.3.0/delta-storage-2.3.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;2.3.0!delta-storage.jar (257ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.8!antlr4-runtime.jar (282ms)\n",
      ":: resolution report :: resolve 4057ms :: artifacts dl 1668ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fa6a474d-3966-4440-a3ef-df689b3e062f\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (4246kB/9ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/11 16:35:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession inicializada com sucesso!\n",
      "25/06/11 16:35:09 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/06/11 16:35:09 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/06/11 16:35:18 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/06/11 16:35:18 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.19.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/11 16:35:21 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/11 16:35:24 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `default`.`test_delta` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/06/11 16:35:24 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/06/11 16:35:24 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "25/06/11 16:35:24 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/06/11 16:35:24 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/06/11 16:35:25 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "Delta Lake configurado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Configuração do SparkSession com Delta Lake\n",
    "builder = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ExplorarTabelas\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/app/spark-warehouse\")\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .config(\"javax.jdo.option.ConnectionURL\", \"jdbc:derby:/app/derby/metastore_db;create=true\")\n",
    "    .config(\"javax.jdo.option.ConnectionDriverName\", \"org.apache.derby.jdbc.EmbeddedDriver\")\n",
    "    .config(\"javax.jdo.option.ConnectionUserName\", \"APP\")\n",
    "    .config(\"javax.jdo.option.ConnectionPassword\", \"mine\")\n",
    "    .enableHiveSupport()\n",
    ")\n",
    "\n",
    "spark = builder.getOrCreate()\n",
    "print(\"SparkSession inicializada com sucesso!\")\n",
    "\n",
    "# Verificar se o Delta Lake está disponível\n",
    "try:\n",
    "    spark.sql(\"CREATE TABLE IF NOT EXISTS test_delta USING delta AS SELECT 1 as id\")\n",
    "    spark.sql(\"DROP TABLE test_delta\")\n",
    "    print(\"Delta Lake configurado com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao verificar Delta Lake: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c293d",
   "metadata": {},
   "source": [
    "## 2. Carregando a Tabela Bronze\n",
    "\n",
    "Lendo os dados da tabela bronze diretamente do formato Delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2c7c145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema da tabela Bronze:\n",
      "root\n",
      " |-- data_inicio: string (nullable = true)\n",
      " |-- data_fim: string (nullable = true)\n",
      " |-- categoria: string (nullable = true)\n",
      " |-- local_inicio: string (nullable = true)\n",
      " |-- local_fim: string (nullable = true)\n",
      " |-- distancia: string (nullable = true)\n",
      " |-- proposito: string (nullable = true)\n",
      "\n",
      "\n",
      "Amostra dos dados:\n",
      "+----------------+----------------+---------+------------+---------------+---------+-----------------+\n",
      "|     data_inicio|        data_fim|categoria|local_inicio|      local_fim|distancia|        proposito|\n",
      "+----------------+----------------+---------+------------+---------------+---------+-----------------+\n",
      "|01-01-2016 21:11|01-01-2016 21:17|  Negocio| Fort Pierce|    Fort Pierce|       51|      Alimentação|\n",
      "|01-02-2016 01:25|01-02-2016 01:37|  Negocio| Fort Pierce|    Fort Pierce|        5|             null|\n",
      "|01-02-2016 20:25|01-02-2016 20:38|  Negocio| Fort Pierce|    Fort Pierce|       48|         Entregas|\n",
      "|01-05-2016 17:31|01-05-2016 17:45|  Negocio| Fort Pierce|    Fort Pierce|       47|          Reunião|\n",
      "|01-06-2016 14:42|01-06-2016 15:49|  Negocio| Fort Pierce|West Palm Beach|      637|Visita ao cliente|\n",
      "+----------------+----------------+---------+------------+---------------+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carregando a tabela Bronze e registrando no catálogo\n",
    "bronze_df = spark.read.format(\"delta\").load(\"/app/data/bronze/b_info_transportes\")\n",
    "bronze_df.createOrReplaceTempView(\"b_info_transportes\")\n",
    "print(\"Schema da tabela Bronze:\")\n",
    "bronze_df.printSchema()\n",
    "\n",
    "print(\"\\nAmostra dos dados:\")\n",
    "bronze_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b1a8b",
   "metadata": {},
   "source": [
    "## 3. Carregando a Tabela Silver\n",
    "\n",
    "Lendo os dados da tabela silver processada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb424f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema da tabela Silver:\n",
      "root\n",
      " |-- data_inicio: string (nullable = true)\n",
      " |-- data_fim: string (nullable = true)\n",
      " |-- categoria: string (nullable = true)\n",
      " |-- local_inicio: string (nullable = true)\n",
      " |-- local_fim: string (nullable = true)\n",
      " |-- distancia: string (nullable = true)\n",
      " |-- proposito: string (nullable = true)\n",
      " |-- dt_refe: date (nullable = true)\n",
      "\n",
      "\n",
      "Amostra dos dados:\n",
      "+----------------+----------------+---------+------------+---------+---------+-----------------+----------+\n",
      "|     data_inicio|        data_fim|categoria|local_inicio|local_fim|distancia|        proposito|   dt_refe|\n",
      "+----------------+----------------+---------+------------+---------+---------+-----------------+----------+\n",
      "|03-04-2016 07:47|03-04-2016 08:06|  negocio|        Cary|   Durham|       99|          reunião|2016-04-03|\n",
      "|03-04-2016 09:46|03-04-2016 10:03|  negocio|      Durham|     Cary|       99|visita ao cliente|2016-04-03|\n",
      "|03-04-2016 11:46|03-04-2016 12:06|  negocio|        Cary|   Durham|      104|          reunião|2016-04-03|\n",
      "|03-04-2016 13:03|03-04-2016 13:25|  negocio|      Durham|     Cary|      109|          reunião|2016-04-03|\n",
      "|03-04-2016 13:40|03-04-2016 14:09|  negocio|        Cary|  Raleigh|      157|visita ao cliente|2016-04-03|\n",
      "+----------------+----------------+---------+------------+---------+---------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carregando a tabela Silver e registrando no catálogo\n",
    "silver_df = spark.read.format(\"delta\").load(\"/app/data/silver/s_info_transportes\")\n",
    "silver_df.createOrReplaceTempView(\"s_info_transportes\")\n",
    "print(\"Schema da tabela Silver:\")\n",
    "silver_df.printSchema()\n",
    "\n",
    "print(\"\\nAmostra dos dados:\")\n",
    "silver_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "314cf8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 30:===>                                                    (1 + 14) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|categoria|\n",
      "+---------+\n",
      "|  pessoal|\n",
      "|  negocio|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#selecione os valores distintos da coluna categoria da tabela Silver sem usar expressions\n",
    "distinct_categories = silver_df.select(\"categoria\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbbea35",
   "metadata": {},
   "source": [
    "## 4. Carregando a Tabela Gold\n",
    "\n",
    "Lendo os dados agregados da tabela gold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67fbcab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema da tabela Gold:\n",
      "root\n",
      " |-- dt_refe: date (nullable = true)\n",
      " |-- qt_corr: integer (nullable = true)\n",
      " |-- qt_corr_neg: integer (nullable = true)\n",
      " |-- qt_corr_pess: integer (nullable = true)\n",
      " |-- vl_max_dist: decimal(17,2) (nullable = true)\n",
      " |-- vl_min_dist: decimal(17,2) (nullable = true)\n",
      " |-- vl_avg_dist: decimal(17,2) (nullable = true)\n",
      " |-- qt_corr_reuni: integer (nullable = true)\n",
      " |-- qt_corr_nao_reuni: integer (nullable = true)\n",
      "\n",
      "\n",
      "Amostra dos dados:\n",
      "+----------+-------+-----------+------------+-----------+-----------+-----------+-------------+-----------------+\n",
      "|   dt_refe|qt_corr|qt_corr_neg|qt_corr_pess|vl_max_dist|vl_min_dist|vl_avg_dist|qt_corr_reuni|qt_corr_nao_reuni|\n",
      "+----------+-------+-----------+------------+-----------+-----------+-----------+-------------+-----------------+\n",
      "|2016-12-03|      2|          1|           1|      22.00|      19.00|      20.50|            0|                0|\n",
      "|2016-11-05|      2|          2|           0|      81.00|     256.00|     168.50|            2|                0|\n",
      "|2016-01-01|      1|          1|           0|      51.00|      51.00|      51.00|            0|                1|\n",
      "|2016-04-07|      4|          4|           0|      99.00|     118.00|      98.00|            3|                1|\n",
      "|2016-10-06|      3|          3|           0|      99.00|     104.00|     122.00|            3|                0|\n",
      "+----------+-------+-----------+------------+-----------+-----------+-----------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carregando a tabela Gold e registrando no catálogo\n",
    "gold_df = spark.read.format(\"delta\").load(\"/app/data/gold/info_corridas_do_dia\")\n",
    "gold_df.createOrReplaceTempView(\"info_corridas_do_dia\")\n",
    "print(\"Schema da tabela Gold:\")\n",
    "gold_df.printSchema()\n",
    "\n",
    "print(\"\\nAmostra dos dados:\")\n",
    "gold_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d90a0064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+------------+-----------+-----------+-----------+-------------+-----------------+\n",
      "|   dt_refe|qt_corr|qt_corr_neg|qt_corr_pess|vl_max_dist|vl_min_dist|vl_avg_dist|qt_corr_reuni|qt_corr_nao_reuni|\n",
      "+----------+-------+-----------+------------+-----------+-----------+-----------+-------------+-----------------+\n",
      "|2016-12-03|      2|          1|           1|      22.00|      19.00|      20.50|            0|                0|\n",
      "|2016-12-07|      3|          1|           2|      87.00|     123.00|      74.67|            0|                0|\n",
      "|2016-02-04|      6|          4|           2|     805.00|     144.00|     595.00|            1|                3|\n",
      "|2016-03-03|      5|          4|           1|      76.00|     173.00|      69.20|            1|                3|\n",
      "|2016-05-03|      6|          4|           2|      78.00|      35.00|      56.17|            0|                4|\n",
      "|2016-01-04|      4|          3|           1|       7.00|      11.00|      94.00|            2|                1|\n",
      "|2016-09-02|      6|          2|           4|      61.00|      15.00|      40.67|            0|                1|\n",
      "|2016-08-03|      3|          2|           1|      76.00|      16.00|      54.67|            0|                2|\n",
      "+----------+-------+-----------+------------+-----------+-----------+-----------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrar uma data na tabela gold_df usando PySpark DataFrame API\n",
    "data_especifica = \"2016-06-09\"\n",
    "\n",
    "filtered_gold_df = gold_df.filter(gold_df[\"QT_CORR_PESS\"] > 0)\n",
    "\n",
    "# Exibir os resultados\n",
    "filtered_gold_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506283a6",
   "metadata": {},
   "source": [
    "## 5. Manipulação dos Dados\n",
    "\n",
    "Exemplos de operações que você pode fazer com os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3d25773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de registros:\n",
      "Bronze: 1,153\n",
      "Silver: 420\n",
      "Gold: 114\n"
     ]
    }
   ],
   "source": [
    "# Exemplo 1: Contagem de registros por tabela\n",
    "print(\"Quantidade de registros:\")\n",
    "print(f\"Bronze: {bronze_df.count():,}\")\n",
    "print(f\"Silver: {silver_df.count():,}\")\n",
    "print(f\"Gold: {gold_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed9086b",
   "metadata": {},
   "source": [
    "## Análise das Partições\n",
    "\n",
    "Vamos explorar as partições da tabela Gold por data de referência:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa9d47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partições da tabela:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 67:===>                                                    (1 + 14) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   DT_REFE|\n",
      "+----------+\n",
      "|2016-01-01|\n",
      "|2016-01-02|\n",
      "|2016-01-03|\n",
      "|2016-01-04|\n",
      "|2016-01-05|\n",
      "|2016-01-06|\n",
      "|2016-01-07|\n",
      "|2016-01-08|\n",
      "|2016-01-09|\n",
      "|2016-01-11|\n",
      "|2016-01-12|\n",
      "|2016-02-01|\n",
      "|2016-02-02|\n",
      "|2016-02-04|\n",
      "|2016-02-05|\n",
      "|2016-02-07|\n",
      "|2016-02-08|\n",
      "|2016-02-09|\n",
      "|2016-02-11|\n",
      "|2016-02-12|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "table_df = spark.table(\"info_corridas_do_dia\")\n",
    "\n",
    "print(\"\\nPartições da tabela:\")\n",
    "table_df.select('DT_REFE').distinct().orderBy('DT_REFE').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f8174e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases disponíveis:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Listar todos os bancos de dados disponíveis\n",
    "print(\"Databases disponíveis:\")\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "729bc21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabelas no banco de dados atual:\n",
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|  default|  b_info_transportes|      false|\n",
      "|  default|info_corridas_do_dia|      false|\n",
      "|  default|  s_info_transportes|      false|\n",
      "|         |  b_info_transportes|      false|\n",
      "|         |info_corridas_do_dia|      false|\n",
      "|         |  s_info_transportes|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Listar todas as tabelas no banco de dados atual\n",
    "print(\"\\nTabelas no banco de dados atual:\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba9d6853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detalhes da tabela Gold:\n",
      "+-----------------+-------------+-------+\n",
      "|col_name         |data_type    |comment|\n",
      "+-----------------+-------------+-------+\n",
      "|dt_refe          |date         |null   |\n",
      "|qt_corr          |int          |null   |\n",
      "|qt_corr_neg      |int          |null   |\n",
      "|qt_corr_pess     |int          |null   |\n",
      "|vl_max_dist      |decimal(17,2)|null   |\n",
      "|vl_min_dist      |decimal(17,2)|null   |\n",
      "|vl_avg_dist      |decimal(17,2)|null   |\n",
      "|qt_corr_reuni    |int          |null   |\n",
      "|qt_corr_nao_reuni|int          |null   |\n",
      "+-----------------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar informações detalhadas sobre uma tabela específica (exemplo com a tabela gold)\n",
    "print(\"\\nDetalhes da tabela Gold:\")\n",
    "spark.sql(\"DESCRIBE EXTENDED info_corridas_do_dia\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95a29b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+------------+-----------+-----------+-----------+-------------+-----------------+\n",
      "|dt_refe   |qt_corr|qt_corr_neg|qt_corr_pess|vl_max_dist|vl_min_dist|vl_avg_dist|qt_corr_reuni|qt_corr_nao_reuni|\n",
      "+----------+-------+-----------+------------+-----------+-----------+-----------+-------------+-----------------+\n",
      "|2016-12-03|2      |1          |1           |22.00      |19.00      |20.50      |0            |0                |\n",
      "|2016-02-05|2      |2          |0           |39.00      |22.00      |30.50      |0            |2                |\n",
      "|2016-01-09|3      |3          |0           |22.00      |106.00     |47.00      |0            |0                |\n",
      "|2016-05-07|5      |5          |0           |99.00      |12.00      |60.40      |1            |2                |\n",
      "|2016-04-12|2      |2          |0           |34.00      |29.00      |31.50      |0            |2                |\n",
      "|2016-01-12|4      |4          |0           |55.00      |29.00      |42.00      |1            |3                |\n",
      "|2016-01-03|2      |2          |0           |8.00       |8.00       |8.00       |1            |1                |\n",
      "|2016-03-07|3      |3          |0           |99.00      |31.00      |53.67      |1            |2                |\n",
      "|2016-02-02|3      |3          |0           |83.00      |16.00      |35.00      |1            |2                |\n",
      "|2016-10-03|4      |4          |0           |84.00      |128.00     |62.75      |2            |0                |\n",
      "|2016-11-05|2      |2          |0           |81.00      |256.00     |168.50     |2            |0                |\n",
      "|2016-05-09|1      |1          |0           |172.00     |172.00     |172.00     |0            |0                |\n",
      "|2016-08-07|4      |4          |0           |77.00      |125.00     |85.25      |0            |0                |\n",
      "|2016-05-12|2      |2          |0           |41.00      |38.00      |39.50      |0            |1                |\n",
      "|2016-12-01|6      |6          |0           |4.00       |151.00     |36.33      |1            |5                |\n",
      "|2016-09-11|8      |8          |0           |9.00       |11.00      |51.38      |1            |1                |\n",
      "|2016-04-11|4      |4          |0           |79.00      |38.00      |49.75      |0            |1                |\n",
      "|2016-12-07|3      |1          |2           |87.00      |123.00     |74.67      |0            |0                |\n",
      "|2016-02-04|6      |4          |2           |805.00     |144.00     |595.00     |1            |3                |\n",
      "|2016-07-01|1      |1          |0           |8.00       |8.00       |8.00       |1            |0                |\n",
      "+----------+-------+-----------+------------+-----------+-----------+-----------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from info_corridas_do_dia where QT_CORR_NEG > 0\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd580e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
