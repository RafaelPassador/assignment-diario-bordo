{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a868f0",
   "metadata": {},
   "source": [
    "# Explorando as Tabelas Delta\n",
    "\n",
    "Este notebook permite explorar e manipular as tabelas Delta do projeto de diário de bordo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1252db",
   "metadata": {},
   "source": [
    "## 1. Configuração do SparkSession\n",
    "\n",
    "Configurando a sessão Spark com suporte ao Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3539879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionURL\n",
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionPassword\n",
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionDriverName\n",
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionUserName\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-15ae6718-4396-4669-9dce-8296795d59df;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.3.0/delta-core_2.12-2.3.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;2.3.0!delta-core_2.12.jar (1188ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/2.3.0/delta-storage-2.3.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;2.3.0!delta-storage.jar (260ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.8!antlr4-runtime.jar (2716ms)\n",
      ":: resolution report :: resolve 4156ms :: artifacts dl 4174ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-15ae6718-4396-4669-9dce-8296795d59df\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (4246kB/14ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/11 13:03:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession inicializada com sucesso!\n",
      "25/06/11 13:03:19 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/06/11 13:03:19 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/06/11 13:04:06 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/06/11 13:04:06 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/11 13:04:13 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/06/11 13:04:17 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `default`.`test_delta` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "25/06/11 13:04:18 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/06/11 13:04:18 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "25/06/11 13:04:18 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/06/11 13:04:18 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/06/11 13:04:19 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "Delta Lake configurado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Configuração do SparkSession com Delta Lake\n",
    "builder = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ExplorarTabelas\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.3.0\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/app/spark-warehouse\")\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .config(\"javax.jdo.option.ConnectionURL\", \"jdbc:derby:/app/derby/metastore_db;create=true\")\n",
    "    .config(\"javax.jdo.option.ConnectionDriverName\", \"org.apache.derby.jdbc.EmbeddedDriver\")\n",
    "    .config(\"javax.jdo.option.ConnectionUserName\", \"APP\")\n",
    "    .config(\"javax.jdo.option.ConnectionPassword\", \"mine\")\n",
    "    .enableHiveSupport()\n",
    ")\n",
    "\n",
    "spark = builder.getOrCreate()\n",
    "print(\"SparkSession inicializada com sucesso!\")\n",
    "\n",
    "# Verificar se o Delta Lake está disponível\n",
    "try:\n",
    "    spark.sql(\"CREATE TABLE IF NOT EXISTS test_delta USING delta AS SELECT 1 as id\")\n",
    "    spark.sql(\"DROP TABLE test_delta\")\n",
    "    print(\"Delta Lake configurado com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao verificar Delta Lake: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098c293d",
   "metadata": {},
   "source": [
    "## 2. Carregando a Tabela Bronze\n",
    "\n",
    "Lendo os dados da tabela bronze diretamente do formato Delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7c145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema da tabela Bronze:\n",
      "root\n",
      " |-- DATA_INICIO: string (nullable = true)\n",
      " |-- DATA_FIM: string (nullable = true)\n",
      " |-- CATEGORIA: string (nullable = true)\n",
      " |-- LOCAL_INICIO: string (nullable = true)\n",
      " |-- LOCAL_FIM: string (nullable = true)\n",
      " |-- DISTANCIA: string (nullable = true)\n",
      " |-- PROPOSITO: string (nullable = true)\n",
      "\n",
      "\n",
      "Amostra dos dados:\n",
      "+----------------+----------------+---------+------------+---------------+---------+-----------------+\n",
      "|     DATA_INICIO|        DATA_FIM|CATEGORIA|LOCAL_INICIO|      LOCAL_FIM|DISTANCIA|        PROPOSITO|\n",
      "+----------------+----------------+---------+------------+---------------+---------+-----------------+\n",
      "|01-01-2016 21:11|01-01-2016 21:17|  Negocio| Fort Pierce|    Fort Pierce|       51|      Alimentação|\n",
      "|01-02-2016 01:25|01-02-2016 01:37|  Negocio| Fort Pierce|    Fort Pierce|        5|             null|\n",
      "|01-02-2016 20:25|01-02-2016 20:38|  Negocio| Fort Pierce|    Fort Pierce|       48|         Entregas|\n",
      "|01-05-2016 17:31|01-05-2016 17:45|  Negocio| Fort Pierce|    Fort Pierce|       47|          Reunião|\n",
      "|01-06-2016 14:42|01-06-2016 15:49|  Negocio| Fort Pierce|West Palm Beach|      637|Visita ao cliente|\n",
      "+----------------+----------------+---------+------------+---------------+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carregando a tabela Bronze e registrando no catálogo\n",
    "bronze_df = spark.read.format(\"delta\").load(\"/app/data/bronze/b_info_transportes\")\n",
    "bronze_df.createOrReplaceTempView(\"b_info_transportes\")\n",
    "print(\"Schema da tabela Bronze:\")\n",
    "bronze_df.printSchema()\n",
    "\n",
    "print(\"\\nAmostra dos dados:\")\n",
    "bronze_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b1a8b",
   "metadata": {},
   "source": [
    "## 3. Carregando a Tabela Silver\n",
    "\n",
    "Lendo os dados da tabela silver processada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb424f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema da tabela Silver:\n",
      "root\n",
      " |-- DATA_INICIO: string (nullable = true)\n",
      " |-- DATA_FIM: string (nullable = true)\n",
      " |-- CATEGORIA: string (nullable = true)\n",
      " |-- LOCAL_INICIO: string (nullable = true)\n",
      " |-- LOCAL_FIM: string (nullable = true)\n",
      " |-- DISTANCIA: string (nullable = true)\n",
      " |-- PROPOSITO: string (nullable = true)\n",
      " |-- DT_REFE: date (nullable = true)\n",
      "\n",
      "\n",
      "Amostra dos dados:\n",
      "+----------------+----------------+---------+----------------+----------+---------+-----------------+----------+\n",
      "|     DATA_INICIO|        DATA_FIM|CATEGORIA|    LOCAL_INICIO| LOCAL_FIM|DISTANCIA|        PROPOSITO|   DT_REFE|\n",
      "+----------------+----------------+---------+----------------+----------+---------+-----------------+----------+\n",
      "|04-01-2016 13:43|04-01-2016 14:01|  negocio|       Kissimmee| Kissimmee|       11|          reunião|2016-01-04|\n",
      "|04-01-2016 14:36|04-01-2016 15:24|  negocio|       Kissimmee|   Orlando|      155|visita ao cliente|2016-01-04|\n",
      "|04-01-2016 16:01|04-01-2016 16:49|  negocio|         Orlando| Kissimmee|      203|          reunião|2016-01-04|\n",
      "|04-01-2016 16:52|04-01-2016 16:57|  pessoal|       Kissimmee| Kissimmee|        7|             null|2016-01-04|\n",
      "|10-11-2016 01:27|10-11-2016 02:08|  negocio|Unknown Location|R?walpindi|      171|          reunião|2016-11-10|\n",
      "+----------------+----------------+---------+----------------+----------+---------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carregando a tabela Silver e registrando no catálogo\n",
    "silver_df = spark.read.format(\"delta\").load(\"/app/data/silver/s_info_transportes\")\n",
    "silver_df.createOrReplaceTempView(\"s_info_transportes\")\n",
    "print(\"Schema da tabela Silver:\")\n",
    "silver_df.printSchema()\n",
    "\n",
    "print(\"\\nAmostra dos dados:\")\n",
    "silver_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314cf8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecione os valores distintos da coluna categoria da tabela Silver sem usar expressions\n",
    "distinct_categories = silver_df.select(\"categoria\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbbea35",
   "metadata": {},
   "source": [
    "## 4. Carregando a Tabela Gold\n",
    "\n",
    "Lendo os dados agregados da tabela gold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67fbcab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema da tabela Gold:\n",
      "root\n",
      " |-- DT_REFE: date (nullable = true)\n",
      " |-- QT_CORR: long (nullable = true)\n",
      " |-- QT_CORR_NEG: long (nullable = true)\n",
      " |-- QT_CORR_PESS: long (nullable = true)\n",
      " |-- VL_MAX_DIST: string (nullable = true)\n",
      " |-- VL_MIN_DIST: string (nullable = true)\n",
      " |-- VL_AVG_DIST: double (nullable = true)\n",
      " |-- QT_CORR_REUNI: long (nullable = true)\n",
      " |-- QT_CORR_NAO_REUNI: long (nullable = true)\n",
      "\n",
      "\n",
      "Amostra dos dados:\n",
      "+----------+-------+-----------+------------+-----------+-----------+-----------+-------------+-----------------+\n",
      "|   DT_REFE|QT_CORR|QT_CORR_NEG|QT_CORR_PESS|VL_MAX_DIST|VL_MIN_DIST|VL_AVG_DIST|QT_CORR_REUNI|QT_CORR_NAO_REUNI|\n",
      "+----------+-------+-----------+------------+-----------+-----------+-----------+-------------+-----------------+\n",
      "|2016-06-09|      1|          0|           0|        691|        691|      691.0|            0|                0|\n",
      "|2016-05-05|      3|          0|           0|         29|        129|      100.0|            1|                2|\n",
      "|2016-03-06|      7|          0|           0|         99|        104|       42.0|            2|                5|\n",
      "|2016-10-12|      4|          0|           0|         31|        156|       86.5|            1|                3|\n",
      "|2016-05-07|      5|          0|           0|         99|         12|       60.4|            1|                2|\n",
      "+----------+-------+-----------+------------+-----------+-----------+-----------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carregando a tabela Gold e registrando no catálogo\n",
    "gold_df = spark.read.format(\"delta\").load(\"/app/data/gold/info_corridas_do_dia\")\n",
    "gold_df.createOrReplaceTempView(\"info_corridas_do_dia\")\n",
    "print(\"Schema da tabela Gold:\")\n",
    "gold_df.printSchema()\n",
    "\n",
    "print(\"\\nAmostra dos dados:\")\n",
    "gold_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d90a0064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+------------+-----------+-----------+------------------+-------------+-----------------+\n",
      "|   DT_REFE|QT_CORR|QT_CORR_NEG|QT_CORR_PESS|VL_MAX_DIST|VL_MIN_DIST|       VL_AVG_DIST|QT_CORR_REUNI|QT_CORR_NAO_REUNI|\n",
      "+----------+-------+-----------+------------+-----------+-----------+------------------+-------------+-----------------+\n",
      "|2016-02-04|      6|          0|           2|        805|        144|             595.0|            1|                3|\n",
      "|2016-12-07|      3|          0|           2|         87|        123| 74.66666666666667|            0|                0|\n",
      "|2016-03-03|      5|          0|           1|         76|        173|              69.2|            1|                3|\n",
      "|2016-09-02|      6|          0|           4|         61|         15|40.666666666666664|            0|                1|\n",
      "|2016-12-03|      2|          0|           1|         22|         19|              20.5|            0|                0|\n",
      "|2016-08-03|      3|          0|           1|         76|         16|54.666666666666664|            0|                2|\n",
      "|2016-05-03|      6|          0|           2|         78|         35|56.166666666666664|            0|                4|\n",
      "|2016-01-04|      4|          0|           1|          7|         11|              94.0|            2|                1|\n",
      "+----------+-------+-----------+------------+-----------+-----------+------------------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrar uma data na tabela gold_df usando PySpark DataFrame API\n",
    "data_especifica = \"2016-06-09\"\n",
    "\n",
    "filtered_gold_df = gold_df.filter(gold_df[\"QT_CORR_PESS\"] > 0)\n",
    "\n",
    "# Exibir os resultados\n",
    "filtered_gold_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506283a6",
   "metadata": {},
   "source": [
    "## 5. Manipulação dos Dados\n",
    "\n",
    "Exemplos de operações que você pode fazer com os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3d25773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de registros:\n",
      "Bronze: 1,153\n",
      "Silver: 420\n",
      "Gold: 114\n"
     ]
    }
   ],
   "source": [
    "# Exemplo 1: Contagem de registros por tabela\n",
    "print(\"Quantidade de registros:\")\n",
    "print(f\"Bronze: {bronze_df.count():,}\")\n",
    "print(f\"Silver: {silver_df.count():,}\")\n",
    "print(f\"Gold: {gold_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed9086b",
   "metadata": {},
   "source": [
    "## Análise das Partições\n",
    "\n",
    "Vamos explorar as partições da tabela Gold por data de referência:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bde0e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método 1: Usando os.listdir para ver as partições no sistema de arquivos\n",
    "import os\n",
    "\n",
    "partitions = os.listdir('/app/data/gold/info_corridas_do_dia')\n",
    "partitions = [p for p in partitions if p.startswith('DT_REFE=')]\n",
    "print(f'Total de partições: {len(partitions)}')\n",
    "print('\\nPrimeiras 10 partições:')\n",
    "for p in sorted(partitions)[:10]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f837c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método 2: Usando Spark SQL para analisar as partições\n",
    "print('\\nAnálise das partições via Spark SQL:')\n",
    "gold_df.select('DT_REFE').distinct().orderBy('DT_REFE').show(10)\n",
    "\n",
    "# Estatísticas por partição\n",
    "print('\\nEstatísticas por partição:')\n",
    "gold_df.groupBy('DT_REFE') \\\n",
    "    .agg({'QTD_CORRIDAS': 'sum'}) \\\n",
    "    .orderBy('DT_REFE') \\\n",
    "    .withColumnRenamed('sum(QTD_CORRIDAS)', 'TOTAL_CORRIDAS') \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método 3: Usando SHOW PARTITIONS do Spark SQL\n",
    "print('\\nUsando SHOW PARTITIONS na tabela Gold:')\n",
    "\n",
    "# Agora podemos usar SHOW PARTITIONS na tabela registrada\n",
    "spark.sql(\"SHOW PARTITIONS info_corridas_do_dia\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baa9d47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usando spark.table() diretamente:\n",
      "\n",
      "Partições da tabela:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 100:===>                                                   (1 + 14) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   DT_REFE|\n",
      "+----------+\n",
      "|2016-01-01|\n",
      "|2016-01-02|\n",
      "|2016-01-03|\n",
      "|2016-01-04|\n",
      "|2016-01-05|\n",
      "|2016-01-06|\n",
      "|2016-01-07|\n",
      "|2016-01-08|\n",
      "|2016-01-09|\n",
      "|2016-01-11|\n",
      "|2016-01-12|\n",
      "|2016-02-01|\n",
      "|2016-02-02|\n",
      "|2016-02-04|\n",
      "|2016-02-05|\n",
      "|2016-02-07|\n",
      "|2016-02-08|\n",
      "|2016-02-09|\n",
      "|2016-02-11|\n",
      "|2016-02-12|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Método 4: Usando spark.table() na tabela registrada\n",
    "print('\\nUsando spark.table() diretamente:')\n",
    "\n",
    "# Agora podemos usar spark.table() porque a tabela está registrada\n",
    "table_df = spark.table(\"info_corridas_do_dia\")\n",
    "\n",
    "# Mostrando as partições\n",
    "print(\"\\nPartições da tabela:\")\n",
    "table_df.select('DT_REFE').distinct().orderBy('DT_REFE').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01a26cb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'display'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtable_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/dataframe.py:1988\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1978\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   1979\u001b[0m \n\u001b[1;32m   1980\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;124;03m[Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[1;32m   1986\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 1988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1989\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   1990\u001b[0m     )\n\u001b[1;32m   1991\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'display'"
     ]
    }
   ],
   "source": [
    "table_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5123672",
   "metadata": {},
   "source": [
    "## 6. Consultas Personalizadas\n",
    "\n",
    "Espaço para suas próprias consultas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fadd4a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Máximo, mínimo e média de QTD_CORRIDAS na tabela Gold:\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Column 'QTD_CORRIDAS' does not exist. Did you mean one of the following? [info_corridas_do_dia.DT_REFE, info_corridas_do_dia.QT_CORR, info_corridas_do_dia.QT_CORR_NEG, info_corridas_do_dia.QT_CORR_PESS, info_corridas_do_dia.QT_CORR_REUNI, info_corridas_do_dia.VL_MIN_DIST, info_corridas_do_dia.VL_AVG_DIST, info_corridas_do_dia.VL_MAX_DIST, info_corridas_do_dia.QT_CORR_NAO_REUNI]; line 3 pos 12;\n'Project ['MAX('QTD_CORRIDAS) AS MAX_CORRIDAS#3295, 'MIN('QTD_CORRIDAS) AS MIN_CORRIDAS#3296, 'AVG('QTD_CORRIDAS) AS MEDIA_CORRIDAS#3297]\n+- SubqueryAlias info_corridas_do_dia\n   +- View (`info_corridas_do_dia`, [DT_REFE#2434,QT_CORR#2435L,QT_CORR_NEG#2436L,QT_CORR_PESS#2437L,VL_MAX_DIST#2438,VL_MIN_DIST#2439,VL_AVG_DIST#2440,QT_CORR_REUNI#2441L,QT_CORR_NAO_REUNI#2442L])\n      +- Relation [DT_REFE#2434,QT_CORR#2435L,QT_CORR_NEG#2436L,QT_CORR_PESS#2437L,VL_MAX_DIST#2438,VL_MIN_DIST#2439,VL_AVG_DIST#2440,QT_CORR_REUNI#2441L,QT_CORR_NAO_REUNI#2442L] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Checar os máximos e minimos da tabela Gold usando pyspark\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMáximo, mínimo e média de QTD_CORRIDAS na tabela Gold:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m stats_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m    SELECT \u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m        MAX(QTD_CORRIDAS) AS MAX_CORRIDAS,\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m        MIN(QTD_CORRIDAS) AS MIN_CORRIDAS,\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m        AVG(QTD_CORRIDAS) AS MEDIA_CORRIDAS\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m    FROM info_corridas_do_dia\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m stats_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     sqlQuery \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mformat(sqlQuery, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column 'QTD_CORRIDAS' does not exist. Did you mean one of the following? [info_corridas_do_dia.DT_REFE, info_corridas_do_dia.QT_CORR, info_corridas_do_dia.QT_CORR_NEG, info_corridas_do_dia.QT_CORR_PESS, info_corridas_do_dia.QT_CORR_REUNI, info_corridas_do_dia.VL_MIN_DIST, info_corridas_do_dia.VL_AVG_DIST, info_corridas_do_dia.VL_MAX_DIST, info_corridas_do_dia.QT_CORR_NAO_REUNI]; line 3 pos 12;\n'Project ['MAX('QTD_CORRIDAS) AS MAX_CORRIDAS#3295, 'MIN('QTD_CORRIDAS) AS MIN_CORRIDAS#3296, 'AVG('QTD_CORRIDAS) AS MEDIA_CORRIDAS#3297]\n+- SubqueryAlias info_corridas_do_dia\n   +- View (`info_corridas_do_dia`, [DT_REFE#2434,QT_CORR#2435L,QT_CORR_NEG#2436L,QT_CORR_PESS#2437L,VL_MAX_DIST#2438,VL_MIN_DIST#2439,VL_AVG_DIST#2440,QT_CORR_REUNI#2441L,QT_CORR_NAO_REUNI#2442L])\n      +- Relation [DT_REFE#2434,QT_CORR#2435L,QT_CORR_NEG#2436L,QT_CORR_PESS#2437L,VL_MAX_DIST#2438,VL_MIN_DIST#2439,VL_AVG_DIST#2440,QT_CORR_REUNI#2441L,QT_CORR_NAO_REUNI#2442L] parquet\n"
     ]
    }
   ],
   "source": [
    "#Checar os máximos e minimos da tabela Gold usando pyspark\n",
    "print(\"\\nMáximo, mínimo e média de QTD_CORRIDAS na tabela Gold:\")\n",
    "stats_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        MAX(QTD_CORRIDAS) AS MAX_CORRIDAS,\n",
    "        MIN(QTD_CORRIDAS) AS MIN_CORRIDAS,\n",
    "        AVG(QTD_CORRIDAS) AS MEDIA_CORRIDAS\n",
    "    FROM info_corridas_do_dia\n",
    "\"\"\")\n",
    "stats_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea16f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f8174e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases disponíveis:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Listar todos os bancos de dados disponíveis\n",
    "print(\"Databases disponíveis:\")\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729bc21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tabelas no banco de dados atual:\n",
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|  default|  b_info_transportes|      false|\n",
      "|  default|info_corridas_do_dia|      false|\n",
      "|  default|  s_info_transportes|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Listar todas as tabelas no banco de dados atual\n",
    "print(\"\\nTabelas no banco de dados atual:\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9d6853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detalhes da tabela Gold:\n",
      "+----------------------------+---------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                          |comment|\n",
      "+----------------------------+---------------------------------------------------+-------+\n",
      "|DT_REFE                     |date                                               |       |\n",
      "|QT_CORR                     |bigint                                             |       |\n",
      "|QT_CORR_NEG                 |bigint                                             |       |\n",
      "|QT_CORR_PESS                |bigint                                             |       |\n",
      "|VL_MAX_DIST                 |string                                             |       |\n",
      "|VL_MIN_DIST                 |string                                             |       |\n",
      "|VL_AVG_DIST                 |double                                             |       |\n",
      "|QT_CORR_REUNI               |bigint                                             |       |\n",
      "|QT_CORR_NAO_REUNI           |bigint                                             |       |\n",
      "|                            |                                                   |       |\n",
      "|# Partitioning              |                                                   |       |\n",
      "|Part 0                      |DT_REFE                                            |       |\n",
      "|                            |                                                   |       |\n",
      "|# Detailed Table Information|                                                   |       |\n",
      "|Name                        |default.info_corridas_do_dia                       |       |\n",
      "|Location                    |file:/app/data/gold/info_corridas_do_dia           |       |\n",
      "|Provider                    |delta                                              |       |\n",
      "|Owner                       |root                                               |       |\n",
      "|External                    |true                                               |       |\n",
      "|Table Properties            |[delta.minReaderVersion=1,delta.minWriterVersion=2]|       |\n",
      "+----------------------------+---------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar informações detalhadas sobre uma tabela específica (exemplo com a tabela gold)\n",
    "print(\"\\nDetalhes da tabela Gold:\")\n",
    "spark.sql(\"DESCRIBE EXTENDED info_corridas_do_dia\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a29b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+------------+-----------+-----------+------------------+-------------+-----------------+\n",
      "|DT_REFE   |QT_CORR|QT_CORR_NEG|QT_CORR_PESS|VL_MAX_DIST|VL_MIN_DIST|VL_AVG_DIST       |QT_CORR_REUNI|QT_CORR_NAO_REUNI|\n",
      "+----------+-------+-----------+------------+-----------+-----------+------------------+-------------+-----------------+\n",
      "|2016-03-04|1      |1          |0           |1593       |1593       |1593.0            |1            |0                |\n",
      "|2016-06-10|4      |4          |0           |184        |1126       |406.75            |0            |0                |\n",
      "|2016-04-10|2      |2          |0           |286        |151        |218.5             |0            |0                |\n",
      "|2016-07-11|2      |2          |0           |132        |118        |125.0             |0            |2                |\n",
      "|2016-12-10|1      |1          |0           |184        |184        |184.0             |0            |0                |\n",
      "|2016-02-04|6      |4          |2           |805        |144        |595.0             |1            |3                |\n",
      "|2016-05-09|1      |1          |0           |172        |172        |172.0             |0            |0                |\n",
      "|2016-11-10|1      |1          |0           |171        |171        |171.0             |1            |0                |\n",
      "|2016-06-09|1      |1          |0           |691        |691        |691.0             |0            |0                |\n",
      "|2016-05-02|2      |2          |0           |104        |104        |104.0             |2            |0                |\n",
      "|2016-09-12|4      |4          |0           |88         |189        |96.0              |1            |3                |\n",
      "|2016-01-02|3      |3          |0           |39         |194        |155.33333333333334|0            |3                |\n",
      "|2016-01-09|3      |3          |0           |22         |106        |47.0              |0            |0                |\n",
      "|2016-03-10|4      |4          |0           |28         |105        |69.0              |0            |0                |\n",
      "|2016-12-07|3      |1          |2           |87         |123        |74.66666666666667 |0            |0                |\n",
      "|2016-04-05|4      |4          |0           |87         |145        |70.5              |0            |4                |\n",
      "|2016-06-05|3      |3          |0           |79         |144        |80.0              |2            |1                |\n",
      "|2016-06-11|5      |5          |0           |439        |18         |108.4             |1            |1                |\n",
      "|2016-02-07|2      |2          |0           |99         |101        |100.0             |2            |0                |\n",
      "|2016-08-11|4      |4          |0           |36         |113        |68.5              |1            |1                |\n",
      "+----------+-------+-----------+------------+-----------+-----------+------------------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from info_corridas_do_dia where QT_CORR_NEG > 0\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9488534e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
