# ğŸš› InfoTransportes Pipeline

Pipeline de ingestÃ£o, transformaÃ§Ã£o e agregaÃ§Ã£o de dados de transportes usando PySpark e Docker.

## ğŸ“‚ OrganizaÃ§Ã£o

- `src/` - CÃ³digo do pipeline
  - `data_reader.py`, `data_writer.py`, `data_processor.py` (modular)
- `data/` - DiretÃ³rio montado localmente com:
  - `input/` â†’ entrada em CSV
  - `bronze/` â†’ dados brutos
  - `silver/` â†’ dados tratados
  - `gold/` â†’ dados agregados
- `.env` - Define os caminhos dinamicamente
- `ValidacaoCamadas.ipynb` - Notebook para validaÃ§Ã£o dos dados
- `tests/` - Testes automatizados com `pytest`
- `docker/` - Infraestrutura Docker

## ğŸš€ Como Executar com Docker

### 1. PrÃ©-requisitos

- Docker e Docker Compose instalados
- Python 3.9+ instalado (apenas para rodar o notebook ou testar localmente)

### 2. Prepare a entrada

Coloque o arquivo CSV em:

```bash
C:\Users\Usuario\Documents\GitHub\assignment-diario-bordo\data\input\info_transportes.csv
```

> â— Crie a pasta `input/` se ela ainda nÃ£o existir.  
> âŒ As pastas `bronze/`, `silver/` e `gold/` **nÃ£o precisam existir** â€“ o Spark criarÃ¡ essas automaticamente.

### 3. Execute o pipeline completo com:

```bash
docker-compose up --build
```

Isso irÃ¡:

- Rodar os testes automatizados
- Executar o pipeline: `CSV -> Bronze -> Silver -> Gold`

### 4. Verifique os dados salvos

As pastas serÃ£o criadas no seu computador em:

```
C:\Users\Usuario\Documents\GitHub\assignment-diario-bordo\data\bronze\
C:\Users\Usuario\Documents\GitHub\assignment-diario-bordo\data\silver\
C:\Users\Usuario\Documents\GitHub\assignment-diario-bordo\data\gold\
```

VocÃª pode inspecionÃ¡-las com:

```bash
ls data/silver/
```

Ou usando o notebook.

## ğŸ“’ Validar dados com Notebook

Execute localmente:

```bash
jupyter notebook src/notebooks/explore_tables.ipynb

ou

docker-compose run --rm -p 8888:8888 spark-job jupyter notebook --ip 0.0.0.0 --allow-root --no-browser /app/src/notebooks    
```

Esse notebook irÃ¡:

- Inicializar Spark
- Ler dados das camadas `bronze`, `silver` e `gold`
- Mostrar o schema e primeiras linhas
- Explorar partiÃ§Ãµes e realizar consultas personalizadas

## âœ… Testes automatizados

Os testes sÃ£o executados automaticamente com `pytest`:

- Leitura e escrita
- Falhas simuladas
- TransformaÃ§Ãµes Silver e Gold

Para rodar localmente:

```bash
pip install -r requirements.txt
pytest tests/ -v
```

Para rodar via Docker:

```bash
docker-compose run --rm spark-job python3 -m pytest tests/ -v
```

## âš™ï¸ CI com GitHub Actions

- Roda os testes a cada push/pull request na branch `main`.

## ğŸ§  Notas TÃ©cnicas

- Particionado por `DT_REFE`
- Dados organizados em camadas Bronze, Silver e Gold
- Docker cria os diretÃ³rios e salva os dados localmente no host
- Estrutura genÃ©rica, extensÃ­vel e escalÃ¡vel

## ğŸ› ï¸ Autor

Rafael Passador Â· Engenharia de Dados Â· 2025